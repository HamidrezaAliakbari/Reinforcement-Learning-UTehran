{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gridworld 4*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    This is a WindyGrid World environment.\n",
    "    You are an agent on an MxN grid and your goal is to reach the terminal\n",
    "    state at the top left or the bottom right corner.\n",
    "    \n",
    "\n",
    "    For example, a 4x4 grid looks as follows:\n",
    "\n",
    "    T  o  o  o\n",
    "    o  x  o  o\n",
    "    o  o  o  o\n",
    "    o  o  o  T\n",
    "\n",
    "    x is your position and T are the two terminal states.\n",
    "\n",
    "    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
    "    Actions going off the edge leave you in your current state.\n",
    "    You receive a reward of -1 at each step until you reach a terminal state.\n",
    "    Notice : the wind is blowing , so you won't always move in the direction you intend.\n",
    "    \"\"\"\"\"\"You dont need to use the AMAlearn package on this hands on session\"\"\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code and observe the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T  o  x  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "{'prob': 1.0}\n",
      "T  o  o  x\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "{'prob': 1.0}\n",
      "T  o  o  x\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "{'prob': 0.8}\n",
      "T  o  x  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "{'prob': 0.8}\n",
      "T  o  x  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "{'prob': 0.8}\n",
      "T  x  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "{'prob': 0.8}\n",
      "T  o  o  o\n",
      "o  x  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "{'prob': 0.8}\n",
      "T  x  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "{'prob': 0.8}\n",
      "T  x  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "{'prob': 0.8}\n",
      "T  x  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "{'prob': 0.2}\n"
     ]
    }
   ],
   "source": [
    "%run WindyGridworld.ipynb\n",
    "\n",
    "env =  WindyGridworldEnv()\n",
    "env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    env._render()\n",
    "    state, reward, done, info = env.step(env.action_space.sample()) # Take a random action\n",
    "    print(info)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the action space and the state space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action space is showin which actios we can take i an itteration, and state space shows howmany and which states we can get in with possible action that we can take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(env.nA)\n",
    "print(env.nS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the following code indicate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.8, 11, -1.0, False), (0.2, 7, -1.0, False)]\n"
     ]
    }
   ],
   "source": [
    "print(env.P[7][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env.P represents the transition probabilities of the environment. env.P[7] is the transition probabilities of 8th state of our enviroment. env.P[7][2] is when we choose to go down.It contains information about the probability, next state, reward and whether or not our action is done. because of windy environment done-action may be different from our -takenaction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reward is 0.0 if we are stuck in the terminal state, else we will get a -1.0 reward. also, done turns true when we are on the last state, terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best policy using the Value Iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    itter = 0\n",
    "    while 1:\n",
    "        itter += 1\n",
    "        delta = 0\n",
    "        for state in range(env.nS):\n",
    "            v = V[state]\n",
    "            states_value = np.zeros(env.nA)\n",
    "            for action in range(env.nA):\n",
    "                temp_t = env.P[state][action]\n",
    "                p_s_sp, sp, r, d = temp_t[0]\n",
    "                temp_sum = (p_s_sp * ( r + discount_factor * V[sp] ))\n",
    "                if len(temp_t) == 2:\n",
    "                    p_s_sp, sp, r, d = temp_t[1]\n",
    "                    states_value[action] = p_s_sp * ( r + discount_factor * V[sp] )  + temp_sum\n",
    "                else:\n",
    "                    states_value[action] = temp_sum\n",
    "            V[state] = np.max(states_value)\n",
    "            delta = max( delta, abs( v - V[state] ) )\n",
    "            \n",
    "        if delta < theta or itter == 10 :\n",
    "            \n",
    "            for state in range(env.nS):\n",
    "                temp_best_action = 0 \n",
    "                for action in range(env.nA):\n",
    "                    temp_t = env.P[state][action]\n",
    "                    p_s_sp, sp, r, d = temp_t[0]\n",
    "                    temp_sum = (p_s_sp * ( r + discount_factor * V[sp] ))\n",
    "                    if len(temp_t) == 2:\n",
    "                        p_s_sp, sp, r, d = temp_t[1]\n",
    "                        V_temp = p_s_sp * ( r + discount_factor * V[sp] )  + temp_sum\n",
    "                    else:\n",
    "                        V_temp = temp_sum\n",
    "                    if action == 0:\n",
    "                        temp_v = V_temp\n",
    "                    bigger_V = [temp_v, V_temp]\n",
    "                    bigger_value = max(bigger_V)\n",
    "                    bigger_index = bigger_V.index(bigger_value)\n",
    "                   # print(V_temp)\n",
    "                    temp_v = bigger_value\n",
    "                    if bigger_index == 1 :\n",
    "                        temp_best_action = action\n",
    "                policy[state][temp_best_action] = 1\n",
    "            break\n",
    "    # Implement!\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print what the best action in each state is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "policy, V = value_iteration(env)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Print value in each state is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state[ 0] value is :0.0\n",
      "\n",
      "state[ 1] value is :-1.249999872\n",
      "\n",
      "state[ 2] value is :-2.4999979008000004\n",
      "\n",
      "state[ 3] value is :-3.7498972160000004\n",
      "\n",
      "state[ 4] value is :-1.5255955456\n",
      "\n",
      "state[ 5] value is :-2.6279950336000004\n",
      "\n",
      "state[ 6] value is :-3.1399946240000003\n",
      "\n",
      "state[ 7] value is :-2.499994624\n",
      "\n",
      "state[ 8] value is :-2.82247630848\n",
      "\n",
      "state[ 9] value is :-3.0099998720000003\n",
      "\n",
      "state[10] value is :-2.0499998720000003\n",
      "\n",
      "state[11] value is :-1.249999872\n",
      "\n",
      "state[12] value is :-3.0\n",
      "\n",
      "state[13] value is :-2.0\n",
      "\n",
      "state[14] value is :-1.0\n",
      "\n",
      "state[15] value is :0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(V.shape [0]):\n",
    "    print('state[' + \"{:2d}\".format(i) + '] value is :' + str(V[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best policy using the Policy Iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    itter = 0\n",
    "    while 1:\n",
    "        itter += 1\n",
    "        delta = 0\n",
    "        for state in range(env.nS):\n",
    "            v = V[state]\n",
    "            temp_v = np.zeros(4)\n",
    "            for action in range(env.nA):\n",
    "                temp_t = env.P[state][action]\n",
    "                #print(temp_t)\n",
    "                p_s_sp, sp, r, d = temp_t[0]\n",
    "                temp_sum = (p_s_sp * ( r + discount_factor * V[sp] ))\n",
    "                if len(temp_t) == 2:\n",
    "                    p_s_sp, sp, r, d = temp_t[1]\n",
    "                    temp_v[action] = p_s_sp * ( r + discount_factor * V[sp] )  + temp_sum\n",
    "                else:\n",
    "                    temp_v[action] = temp_sum\n",
    "            ts = 0\n",
    "            for action in range(4):\n",
    "                ts += policy[state][action]*temp_v[action]/itter\n",
    "            V[state] = v*(itter-1)/itter + ts\n",
    "            delta = max( delta, abs( v - V[state] ) )\n",
    "            \n",
    "        if delta < theta :\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state[ 0] value is :0.0\n",
      "\n",
      "state[ 1] value is :-1.2492731736636988\n",
      "\n",
      "state[ 2] value is :-2.493903475618819\n",
      "\n",
      "state[ 3] value is :-3.7233223035698617\n",
      "\n",
      "state[ 4] value is :-1.5499196707232803\n",
      "\n",
      "state[ 5] value is :-2.7803460022637165\n",
      "\n",
      "state[ 6] value is :-3.1276749941709414\n",
      "\n",
      "state[ 7] value is :-2.4911640155094306\n",
      "\n",
      "state[ 8] value is :-2.8261628579871396\n",
      "\n",
      "state[ 9] value is :-3.0035542047627515\n",
      "\n",
      "state[10] value is :-2.0487877367705023\n",
      "\n",
      "state[11] value is :-1.2492731736636988\n",
      "\n",
      "state[12] value is :-2.993941608042092\n",
      "\n",
      "state[13] value is :-1.9993932038834965\n",
      "\n",
      "state[14] value is :-1.0\n",
      "\n",
      "state[15] value is :0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "V=policy_eval(policy, env)\n",
    "for i in range(V.shape [0]):\n",
    "    print('state[' + \"{:2d}\".format(i) + '] value is :' + str(V[i]) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval_1(V,policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    while True:\n",
    "            delta = 0\n",
    "            for state in range(env.nS):\n",
    "                v = 0\n",
    "                v_temp = V[state]\n",
    "                states_value = 0\n",
    "                for action, action_probability in enumerate(policy[state]):\n",
    "                    for p_s_ns, next_state, state_reward, done in env.P[state][action]:\n",
    "                        states_value += action_probability * p_s_ns * (state_reward + discount_factor * V[next_state])\n",
    "                v = states_value\n",
    "                delta = max(delta, np.abs(v_temp - v))\n",
    "                V[state] = v\n",
    "            if delta < theta:\n",
    "                    break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_actions = []\n",
    "\n",
    "def policy_improvement(env, policy_eval_fn=policy_eval_1, discount_factor=1.0):\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    states_value = np.zeros(env.nS)\n",
    "    while True:\n",
    "            states_values = policy_eval_1(states_value ,policy, env)\n",
    "            policy_stable = True\n",
    "            for state in range(env.nS):\n",
    "                action_values = np.zeros(env.nA)\n",
    "                chosen_action = np.argmax(policy[state])\n",
    "                for action in range(env.nA):\n",
    "                    for p_s_ns, next_state, state_reward, done in env.P[state][action]:\n",
    "                        action_values[action] += p_s_ns * (state_reward + discount_factor * states_value[next_state])\n",
    "                best_action = np.argmax(action_values)\n",
    "                if  chosen_action != best_action:\n",
    "                    policy_stable = False\n",
    "                policy[state] = np.eye(env.nA)[best_action]\n",
    "                best_actions.append(best_action)\n",
    "            if policy_stable :\n",
    "                return policy ,states_value \n",
    "    #return policy, np.zeros(env.nS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print what the best action in each state is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For state number 0  0\n",
      "\n",
      "For state number 1  3\n",
      "\n",
      "For state number 2  3\n",
      "\n",
      "For state number 3  2\n",
      "\n",
      "For state number 4  0\n",
      "\n",
      "For state number 5  3\n",
      "\n",
      "For state number 6  2\n",
      "\n",
      "For state number 7  2\n",
      "\n",
      "For state number 8  0\n",
      "\n",
      "For state number 9  2\n",
      "\n",
      "For state number 10  2\n",
      "\n",
      "For state number 11  2\n",
      "\n",
      "For state number 12  1\n",
      "\n",
      "For state number 13  1\n",
      "\n",
      "For state number 14  1\n",
      "\n",
      "For state number 15  0\n",
      "\n",
      "For state number 16  0\n",
      "\n",
      "For state number 17  3\n",
      "\n",
      "For state number 18  3\n",
      "\n",
      "For state number 19  3\n",
      "\n",
      "For state number 20  0\n",
      "\n",
      "For state number 21  0\n",
      "\n",
      "For state number 22  2\n",
      "\n",
      "For state number 23  2\n",
      "\n",
      "For state number 24  0\n",
      "\n",
      "For state number 25  2\n",
      "\n",
      "For state number 26  2\n",
      "\n",
      "For state number 27  2\n",
      "\n",
      "For state number 28  1\n",
      "\n",
      "For state number 29  1\n",
      "\n",
      "For state number 30  1\n",
      "\n",
      "For state number 31  0\n",
      "\n",
      "For state number 32  0\n",
      "\n",
      "For state number 33  3\n",
      "\n",
      "For state number 34  3\n",
      "\n",
      "For state number 35  3\n",
      "\n",
      "For state number 36  0\n",
      "\n",
      "For state number 37  0\n",
      "\n",
      "For state number 38  2\n",
      "\n",
      "For state number 39  2\n",
      "\n",
      "For state number 40  0\n",
      "\n",
      "For state number 41  2\n",
      "\n",
      "For state number 42  2\n",
      "\n",
      "For state number 43  2\n",
      "\n",
      "For state number 44  1\n",
      "\n",
      "For state number 45  1\n",
      "\n",
      "For state number 46  1\n",
      "\n",
      "For state number 47  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, V = policy_improvement(env)\n",
    "for i in range(len(best_actions)):\n",
    "    print('For state number ' +str(i) + \" \"+\"{:2d}\".format(best_actions[i]) +  '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print value in each state is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -1.25      , -2.5       , -3.75000001, -1.5256    ,\n",
       "       -2.628     , -3.14      , -2.5       , -2.82248   , -3.01      ,\n",
       "       -2.05      , -1.25      , -3.        , -2.        , -1.        ,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
